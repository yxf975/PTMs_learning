{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmRtfDbBCayc"
   },
   "source": [
    "# Quora Question Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6894,
     "status": "ok",
     "timestamp": 1606858265559,
     "user": {
      "displayName": "Xuefeng Yin",
      "photoUrl": "https://lh3.googleusercontent.com/-ddmqurum4Dk/AAAAAAAAAAI/AAAAAAAAAA4/H1iukjidaN0/s64/photo.jpg",
      "userId": "01601649717182370319"
     },
     "user_tz": -60
    },
    "id": "0oMS4LB1QSU4",
    "outputId": "795389e1-7795-40b6-f762-fa5b38756dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: ['GeForce GTX 1050 Ti']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "\n",
    "    print('There are %d GPU(s) available.' % n_gpu)\n",
    "\n",
    "    print('We will use the GPU:', [torch.cuda.get_device_name(i) for i in range(n_gpu)])\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_odHnVfHi3AF"
   },
   "source": [
    "## all required package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QdH6iHRtC4Hr"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#import bert tokenizer\n",
    "from transformers import  BertTokenizer\n",
    "#import bert classification for finetuning\n",
    "from transformers import BertForSequenceClassification\n",
    "# import adamw optimizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 749,
     "status": "ok",
     "timestamp": 1606860554878,
     "user": {
      "displayName": "Xuefeng Yin",
      "photoUrl": "https://lh3.googleusercontent.com/-ddmqurum4Dk/AAAAAAAAAAI/AAAAAAAAAA4/H1iukjidaN0/s64/photo.jpg",
      "userId": "01601649717182370319"
     },
     "user_tz": -60
    },
    "id": "0k-8PolSC8ei",
    "outputId": "5aa4f1dd-2e92-4c03-d0a3-820cf19875d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    qid1  qid2                                          question1  \\\n",
       "id                                                                  \n",
       "0      1     2  What is the step by step guide to invest in sh...   \n",
       "1      3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2      5     6  How can I increase the speed of my internet co...   \n",
       "3      7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4      9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5     11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "id                                                                   \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  \n",
       "5   I'm a triple Capricorn (Sun, Moon and ascendan...             1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\", index_col=\"id\",nrows=1000)\n",
    "train_data.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question1  \\\n",
       "test_id                                                      \n",
       "0        How does the Surface Pro himself 4 compare wit...   \n",
       "1        Should I have a hair transplant at age 24? How...   \n",
       "2        What but is the best way to send money from Ch...   \n",
       "3                              Which food not emulsifiers?   \n",
       "4                         How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                                 question2  \n",
       "test_id                                                     \n",
       "0        Why did Microsoft choose core m3 and not core ...  \n",
       "1              How much cost does hair transplant require?  \n",
       "2                            What you send money to China?  \n",
       "3                                        What foods fibre?  \n",
       "4                           How their can I start reading?  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"./data/test.csv\", index_col=\"test_id\",nrows=1000)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1606860557693,
     "user": {
      "displayName": "Xuefeng Yin",
      "photoUrl": "https://lh3.googleusercontent.com/-ddmqurum4Dk/AAAAAAAAAAI/AAAAAAAAAA4/H1iukjidaN0/s64/photo.jpg",
      "userId": "01601649717182370319"
     },
     "user_tz": -60
    },
    "id": "Cqby8d78DIOM",
    "outputId": "69168d54-a53d-4304-ec76-fff52caeb9b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>How do I see \"sent invitations\" on Linkedin if...</td>\n",
       "      <td>How can you personalize a LinkedIn invitation?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>How do you make life suit you and stop life fr...</td>\n",
       "      <td>Why are emotionally abusive people in my life?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Why can't I stop watching porn?</td>\n",
       "      <td>Why should / shouldn't I watch porn?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>How do I lose weight without doing any sport?</td>\n",
       "      <td>How do I lose weight without doing exercise or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>Which payment gateway in Saudi that using to c...</td>\n",
       "      <td>Which are all the stress free, relatively easy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question1  \\\n",
       "id                                                       \n",
       "330  How do I see \"sent invitations\" on Linkedin if...   \n",
       "169  How do you make life suit you and stop life fr...   \n",
       "419                    Why can't I stop watching porn?   \n",
       "849      How do I lose weight without doing any sport?   \n",
       "651  Which payment gateway in Saudi that using to c...   \n",
       "\n",
       "                                             question2  \n",
       "id                                                      \n",
       "330  How can you personalize a LinkedIn invitation?...  \n",
       "169  Why are emotionally abusive people in my life?...  \n",
       "419               Why should / shouldn't I watch porn?  \n",
       "849  How do I lose weight without doing exercise or...  \n",
       "651  Which are all the stress free, relatively easy...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_validation data split\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data[[\"question1\", \"question2\"]], \n",
    "                                                    train_data[\"is_duplicate\"], test_size=0.2, random_state=405633)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bYEvdN_D1kN"
   },
   "source": [
    "## convert data to Bert input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uzvzJhtWEKFr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length of the input: 92\n",
      "max token length for BERT: 32\n"
     ]
    }
   ],
   "source": [
    "# load bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "#calculate the maximum sentence length\n",
    "max_len  = 0\n",
    "for _, row in train_data.iterrows():\n",
    "    max_len = max(max_len, len(tokenizer(row['question1'],row['question2'])[\"input_ids\"]))\n",
    "\n",
    "print(\"max token length of the input:\", max_len)\n",
    "    \n",
    "# set the maximum token length\n",
    "max_length = pow(2,int(np.log2(max_len)-1))\n",
    "print(\"max token length for BERT:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "uy0ZyU_0D8Tm"
   },
   "outputs": [],
   "source": [
    "# func to convert data to bert input\n",
    "def convert_to_dataset_torch(data: pd.DataFrame, labels = pd.Series(data=None)) -> TensorDataset:\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        encoded_dict = tokenizer.encode_plus(row[\"question1\"], row[\"question2\"], max_length=max_length, pad_to_max_length=True, \n",
    "                      return_attention_mask=True, return_tensors='pt', truncation=True)\n",
    "        # Add the encoded sentences to the list.\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        token_type_ids.append(encoded_dict[\"token_type_ids\"])\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    if labels.empty:\n",
    "        return TensorDataset(input_ids, attention_masks, token_type_ids)\n",
    "    else:\n",
    "        labels = torch.tensor(labels.values)\n",
    "        return TensorDataset(input_ids, attention_masks, token_type_ids, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7945,
     "status": "ok",
     "timestamp": 1606860574006,
     "user": {
      "displayName": "Xuefeng Yin",
      "photoUrl": "https://lh3.googleusercontent.com/-ddmqurum4Dk/AAAAAAAAAAI/AAAAAAAAAA4/H1iukjidaN0/s64/photo.jpg",
      "userId": "01601649717182370319"
     },
     "user_tz": -60
    },
    "id": "Mg0AKrt6EuRS",
    "outputId": "75bb3fd5-e373-4f2f-92e6-9c3842824760"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/800 [00:00<?, ?it/s]d:\\python\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 800/800 [00:00<00:00, 1069.24it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 1205.36it/s]\n"
     ]
    }
   ],
   "source": [
    "train = convert_to_dataset_torch(data=X_train, labels=y_train)\n",
    "validation = convert_to_dataset_torch(data=X_val, labels= y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlqunAwEMTUa"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "7INEdxNGMVVP"
   },
   "outputs": [],
   "source": [
    "# set batch size for DataLoader(options from paper:16 or 32)\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders for training and validation sets\n",
    "train_dataloader = DataLoader(\n",
    "            train,  \n",
    "            sampler = RandomSampler(train), # Select batches randomly\n",
    "            batch_size = batch_size \n",
    "        )\n",
    "\n",
    "# For validation\n",
    "validation_dataloader = DataLoader(\n",
    "            validation, \n",
    "            sampler = SequentialSampler(validation), # Pull out batches sequentially.\n",
    "            batch_size = batch_size \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8900,
     "status": "ok",
     "timestamp": 1606860578482,
     "user": {
      "displayName": "Xuefeng Yin",
      "photoUrl": "https://lh3.googleusercontent.com/-ddmqurum4Dk/AAAAAAAAAAI/AAAAAAAAAA4/H1iukjidaN0/s64/photo.jpg",
      "userId": "01601649717182370319"
     },
     "user_tz": -60
    },
    "id": "EWByYxRoM7fv",
    "outputId": "93ef1f71-b092-4536-ed7c-30bf679e10cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels=2, # The number of output labels--2  \n",
    "    output_attentions=False, # Whether returns attentions weights.\n",
    "    output_hidden_states=False, # Whether returns all hidden-states.\n",
    ")\n",
    "model.cuda()\n",
    "if n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "eNf2BJ-fNRrK"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate\n",
    "                  eps = 1e-8 # args.adam_epsilon\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "x8uSZwxMNWNu"
   },
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "pQaeob3rNf1l"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "8yCtqeNeH8oo"
   },
   "outputs": [],
   "source": [
    "def fit_batch(dataloader, model, optimizer, epoch):\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=f\"Training epoch:{epoch+1}\", unit=\"batch\"):\n",
    "        # Unpack batch from dataloader.\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        token_type_ids = batch[2].to(device)\n",
    "        labels = batch[3].to(device)\n",
    "        \n",
    "        # clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(input_ids, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        attention_mask=attention_masks, \n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # normlization of the gradients to 1.0 to avoid exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "    return total_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "aOoWt5MnHpLh"
   },
   "outputs": [],
   "source": [
    "def eval_batch(dataloader, model, metric=accuracy_score):\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    predictions , predicted_labels = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "        # Unpack batch from dataloader.\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        token_type_ids = batch[2].to(device)\n",
    "        labels = batch[3].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            outputs = model(input_ids, \n",
    "                            token_type_ids=token_type_ids, \n",
    "                            attention_mask=attention_masks,\n",
    "                            labels=labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "        total_eval_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of validation sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        y_pred = np.argmax(logits, axis=1).flatten()\n",
    "        total_eval_accuracy += metric(label_ids, y_pred)\n",
    "        \n",
    "        predictions.extend(logits.tolist())\n",
    "        predicted_labels.extend(y_pred.tolist())\n",
    "    \n",
    "    return total_eval_accuracy, total_eval_loss, predictions ,predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "IiyBzIMGLCYI"
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, validation_dataloader, model, optimizer, epochs):\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "    \n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        # Put the model into training mode. \n",
    "        model.train()\n",
    "        \n",
    "        total_train_loss = fit_batch(train_dataloader, model, optimizer, epoch)\n",
    "        \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        \n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "        \n",
    "\n",
    "        total_eval_accuracy, total_eval_loss, _, _ = eval_batch(validation_dataloader, model)\n",
    "        \n",
    "        # Report the final accuracy for this validation run.\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"\\n\")\n",
    "        print(f\"score: {avg_val_accuracy}\")\n",
    "    \n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "        print(f\"Validation Loss: {avg_val_loss}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. score.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(f\"Total training took {format_time(time.time()-total_t0)}\")\n",
    "    return training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader, model):\n",
    "    prediction = list()\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"predicting\", unit=\"batch\"):\n",
    "        # Unpack batch from dataloader.\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        token_type_ids = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            outputs = model(input_ids, \n",
    "                            token_type_ids=token_type_ids, \n",
    "                            attention_mask=attention_masks)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        \n",
    "        prediction.append(logits)\n",
    "        \n",
    "    pred_logits = np.concatenate(prediction, axis=0)\n",
    "    pred_label = np.argmax(pred_logits, axis=1).flatten()\n",
    "    print(\"done\")\n",
    "    return (pred_label,pred_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63239,
     "status": "ok",
     "timestamp": 1606860641265,
     "user": {
      "displayName": "Xuefeng Yin",
      "photoUrl": "https://lh3.googleusercontent.com/-ddmqurum4Dk/AAAAAAAAAAI/AAAAAAAAAA4/H1iukjidaN0/s64/photo.jpg",
      "userId": "01601649717182370319"
     },
     "user_tz": -60
    },
    "id": "ZPL2tNvnMQi5",
    "outputId": "48df8250-962e-4b96-88b5-4dfbf0d7ea06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch:1: 100%|███████████████████████████████████████████████████████████| 100/100 [00:23<00:00,  4.32batch/s]\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████| 25/25 [00:01<00:00, 23.91batch/s]\n",
      "Training epoch:2:   1%|▌                                                            | 1/100 [00:00<00:18,  5.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "score: 0.69\n",
      "Validation Loss: 0.547390798330307\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch:2: 100%|███████████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.46batch/s]\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 27.34batch/s]\n",
      "Training epoch:3:   1%|▌                                                            | 1/100 [00:00<00:17,  5.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "score: 0.745\n",
      "Validation Loss: 0.5127146875858307\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch:3: 100%|███████████████████████████████████████████████████████████| 100/100 [00:22<00:00,  4.52batch/s]\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████| 25/25 [00:01<00:00, 23.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "score: 0.745\n",
      "Validation Loss: 0.6192037010192871\n",
      "\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:01:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 405633\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = train(train_dataloader, validation_dataloader, model, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1606860681349,
     "user": {
      "displayName": "Xuefeng Yin",
      "photoUrl": "https://lh3.googleusercontent.com/-ddmqurum4Dk/AAAAAAAAAAI/AAAAAAAAAA4/H1iukjidaN0/s64/photo.jpg",
      "userId": "01601649717182370319"
     },
     "user_tz": -60
    },
    "id": "vxYX6VI7QTrt",
    "outputId": "05493f50-3a4b-4fd6-8f78-842a6bbcba5f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Valid. score.</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.623769</td>\n",
       "      <td>0.547391</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0:00:23</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.440350</td>\n",
       "      <td>0.512715</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0:00:22</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.307956</td>\n",
       "      <td>0.619204</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0:00:22</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss  Valid. score. Training Time Validation Time\n",
       "epoch                                                                         \n",
       "0           0.623769     0.547391          0.690       0:00:23         0:00:01\n",
       "1           0.440350     0.512715          0.745       0:00:22         0:00:01\n",
       "2           0.307956     0.619204          0.745       0:00:22         0:00:01"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats = pd.DataFrame(training_stats).set_index('epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predition for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1000 [00:00<?, ?it/s]d:\\python\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1155.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the DataLoader for test data.\n",
    "prediction_data = convert_to_dataset_torch(test_data)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predicting: 100%|█████████████████████████████████████████████████████████████████| 125/125 [00:04<00:00, 28.77batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred,logits = predict(prediction_dataloader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9711, 0.0289],\n",
       "        [0.8511, 0.1489],\n",
       "        [0.1098, 0.8902],\n",
       "        [0.9599, 0.0401],\n",
       "        [0.0837, 0.9163],\n",
       "        [0.9650, 0.0350],\n",
       "        [0.0459, 0.9541],\n",
       "        [0.7414, 0.2586],\n",
       "        [0.1790, 0.8210],\n",
       "        [0.9490, 0.0510]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = torch.nn.functional.softmax(torch.tensor(logits))\n",
    "prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1], dtype=int64)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOLmcx3F4vXKWveUzbcfpiD",
   "machine_shape": "hm",
   "mount_file_id": "1xfh4748i09xPJSvozJaydh_iS7SxISPv",
   "name": "Question_Pair_Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
